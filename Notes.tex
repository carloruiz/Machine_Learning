\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb}
\usepackage{bbm}
\usepackage{comment}
\usepackage{relsize}
\usepackage{listings}
\usepackage{svg}



\begin{document}

\title{Machine Learning Notes}
\maketitle

\section*{I. Necessary background n sh't}
	\subsection*{A. Distance measures}
		When we think of distance in space, our brains usually default to euclidean distance, or straight line distance. 
		However, there are actually infinitely many different ways to measure distance. In this section we briefly
		note some of the more useful distance measures in machine learning. \\
		
		\noindent \textbf{norm:}\textit{ a function that assigns a strictly positive length or size to each vector in a 
		vector space, save $\vec{0}$, which is assigned a length of zero.} (for a formal definition, google it.) \\ \\
    		
		\indent \textbf{Euclidean Norm:} The one we are all used to (straight line distance), 
		\indent denoted by $ |\vec x|_2 $, $ ||\vec x||_2$, $L_2$, or $\ell_2 $. For a given n-dimensional vector $\vec x$
			$$  \mathlarger{||\vec x||_2 = \sqrt {\sum_{i=1}^n \vec x_i^2} } $$
			
		\indent \textbf{Manhattan Norm:} denoted by subscript 1 ($ ||\vec x||_1$,  $\ell_1$, etc ), the distance 
		\indent a taxi has to drive in a rectangular street grid to get from the origin to 
		\indent the point x. For a given n-dimensional vector $\vec x$
		$$  \mathlarger{ ||\vec x||_1 = \sum_{i=1}^n | x_i | }$$
		
		\textbf{p-norm:} a generalization of the norms. $p = 1$ gives manhattan norm \\
		\indent and $p=2$ gives euclidean norm. 
		$$ \mathlarger{ ||\vec x||_p = (\sum_{i=1}^n | x_i |^p)^{1/p}} $$
		\indent as $p \rightarrow \infty $, the p-norm approaches the infinity norm, or maximum norm:
		\indent $ \left\|\mathbf {x} \right\|_{\infty }:=\max _{i}\left|x_{i}\right|. $
		TODO add figure (maybe more distance measures)
		\subsection*{B. Calculus and Optimization}
			TODO
		\subsection*{C. Problems in Statistical Learning}
			\begin{itemize}
				\item Regression: making a real valued prediction (e.g. predict house value given sq. feet, 
					number of beds, number of baths) Examples are linear regression, k-NN where labels 
					are continuous (we use average of k nearest neighbors to determine y), regression tree. 
				\item Classification: predict a value within a finite set of possibilities. Examples are k-NN with 
					plurality of k nearest neighbors, logistic regression, decision tree. 
				\item Ranking: Put a set of objects in order of relevance. 
			\end{itemize}
		\subsection*{C. Probability}
			Probability can be approached with two mindsets. One views probability theory as the study of 
			frequencies and hence, is called the frequentist approach. A more general approach is called 
			Bayesian. It views probability as a quantification of uncertainty. Jaynes 2003 showed that 
			a Bayesian approach to probability could be regarded as an extension of boolean logic to
			situations involving uncertainty. The Bayesian approach is particularly useful in machine 
			learning. \\
			
			\noindent Let $X, \, Y$ denote events. Then we have, \\ \\
			\indent \indent \textbf{ sum rule:} \indent $\mathlarger{ P(X) = \sum_Y P(X,Y) }$ \\
			\indent \textbf{product rule: } $ P(X,Y) = P(Y \, | \, X)P(X) $ \\ \\
			\noindent applying the product rule twice, we arrive at \\ \\
			\indent \textbf{Bayes theorem:} $\mathlarger {P(Y \, | \, X) = \frac{P(X \, | \, Y)P(Y)}{P(X)}}$ \\ \\
			\noindent In the above, $P(X)$ is called the prior and $P(X \, | \, Y)$ is called the posterior. 
			\noindent We also define \\ \\
			 \textbf{Independence: } $P(X \, | \, Y) = P(X) \implies P(X,Y) = P(X)P(Y)$ \\
			
			1. Sample space, Random variables. Expectation, variance, covariance
			2. Probability distributions (mass vs density). 
			3. Examples, bernoulli, poisson, gaussian.  
							
		\subsection*{D. Statistics:}
			TODO what is a statistic? \\ \\
			A \textit{statistical model} $\mathcal{P} = \{ P_\theta \, : \, \vec \theta \in \Theta \}$ is a family of 
			probability distributions indexed by a set $\Theta$ called the parameter space. In a \textit{parametric 
			statistical model}, the distributions are indexed by a finite number of parameters 
			(i.e. $\Theta \subset \mathbb{R}^k$ for some $ k < \infty$). Examples: Bernoulli, Poisson, 
			Gaussian, Multivariate Gaussian (MVD) distributions. \\
			
			\noindent \textbf{Model fitting}: Suppose we have an observation $x$, and a parametric model 
			$\mathcal P$ with parameter space $\Theta$. Pick some $P_\theta \in \mathcal P$ (i.e. some 
			$\vec \theta \in \Theta$) that best "fits" the observation $x$. \\
			
			\noindent \textbf{Maximum Likelihood Estimation:}  The most popular model fitting method.
			Say we have set $\mathbf X$ of points drawn from some distribution $P_{\vec \theta}$ (see 
			next section). However, we don't know $\vec \theta$. How can we estimate it? Consider again Bayes Theorem: 
			$$ P(\vec \theta \, | \, \mathbf{X}) = \frac{P(\mathbf{X} \, | \, \vec \theta) P(\vec \theta)}{P(\mathbf X)} $$
				
			\noindent If we had no data points, we would want to chose $arg \, max(P(\vec \theta)) $, the $\vec \theta$ 
			that maximizes the prior. However, given we have seen data points $\mathbf{X}$, we want to maximize 
			the posterior:
			$$arg_\theta \, max (P(\vec \theta \, | \, \mathbf{X})) = 
			arg_\theta \, max (\frac{P(\mathbf{X} \, | \, \vec \theta)P(\vec \theta)}{P(\mathbf{X})}) = 
			arg_\theta \, max(P(\mathbf{X} \, | \, \vec \theta)P(\vec \theta)) $$
				
			\noindent We make the further assumption that $P(\vec \theta)$ is uniformly distributed over $\Theta$. 
			Finally, we arrive at:
			$$arg_\theta \, max \, P(\mathbf{X} \, | \, \vec \theta) $$ 
			
			\noindent $P(\mathbf{X} \, | \, \vec \theta)$ is called the \textit{likelihood function} $\mathcal{L}$. 
			This is why this method is called maximum likelihood estimation. To greatly simplify calculations, 
			often allowing us to arrive at a closed form solution, we make the further assumption that the 
			$x$'s in $\mathbf{X}$ are independent. Note that this assumption is not strictly necessary. We have 
			already assumed the points are drawn from the same distribution. Taking these
			two assumptions together, we say the $x_i$'s are i.i.d or independent identically distributed. We can write 
			$$\mathcal{L} = P(\mathbf{X} \, | \, \vec \theta) = \prod_{i=1}^{n}P(x_i \, | \, \vec \theta)$$
			Since $\mathcal L$ is a monotonic function, taking the log of $\mathcal L$ does not change the maximizing 
			$\vec \theta$ but it does greatly simplify calculations since it allows us to write:
			$$ln \, \mathcal L = \sum_{i=1}^{n} ln (P(x_i \, | \, \vec \theta))$$
			
			
			 Parametric vs Non-Parametric \\
			 Linear vs Non-Linear
			 
		\subsection*{E. Classifiers}
			Fundamental statistical learning theory assumption: \textit{labeled examples come from the same
			source as future examples}. More formally $ \{(\vec x, y)\}_i^n$ is an i.i.d sample from a probability
			distribution over $\mathcal{X} \times \mathcal{Y} $ \\ \\
			let $\hat f_D$ be a classifier trained on data set D. 
			$$ \textrm{err}_D(\hat f) \, := \ \lvert\  \{ (\vec x, y) \in D \, | \, \hat f(\vec x) \not = y\} \, \rvert $$ 
			
			\noindent further, define the \textit{true} error rate of classifier $f: \mathcal X\rightarrow \mathcal Y $\\
			$$\textrm{err}_{\mathcal{P}}(f) := P(f(\vec x) \not = y) $$
			note that the true error is with respect to a probability distribution $\mathcal P$ not a set $D$ and 
			$ (\vec x,  y) \sim \mathcal  P$\\
			
			\noindent \textbf{Bayes Optimal Classifier:} given data point $\vec x$ along with labels $Y$,
			$$ f^*(\vec x) = arg_y \, max \ P(Y = y\, | \, \vec x ) = arg_y \, max \ P(\vec x \, | \, Y=y)P(Y = y)$$
			$P(Y=y_i)$, denoted by $\pi_i$ is called the class prior. $P(\vec x \, | \, Y=y)$, denoted by $P_i(x)$,
			is called the class conditional. In layman terms, the Bayes Optimal classifier always return the 
			label with the highest probability of being correct. \\\
			
			\noindent \textbf{Naive Bayes Classifier:} makes the assumption that the individual features 
			are independent of each other given the class label. Hence the term naive. We can write 
			the naive Bayes Classifier as:
			$$ f^*(\vec x) =  arg_y \, max \ \prod_{j=1}^{d}P(\vec x_j \, | \, Y=y)P(Y = y)$$
			Naive Bayes Classifier is computationally very simple and it's quick to code. However, if the data
			set in question has correlated features (many do) it can give very bad estimates. \\
			
			\noindent \textbf{Generative Models:} In the context of classification problems, a generative 
			model is a statistical model $\mathcal P$ on $\mathcal X \times \{1,2,3, ..., k\}$ where each
			$P_\theta \in \mathcal P$ is 
			$$P_\theta(\vec x,y) = \pi_y \cdot P_y(\vec x) $$
			$$ \vec \theta = (\pi_1, \pi_2, ..., \pi_k, P_1, P_2, ...P_k)$$
			\\
			This theory is good an all, but how do we actually go about building a classifier? Say we have
			a data set $D = \{ (x^{(i)}, y^{(i)})\}_{i=1}^n$ regarded as an i.i.d. sample.
			\begin{enumerate}
				\item Partition $\{x^{(i)}\}_{i=1}^{n}$ into $D_1, D_2, ... D_k$ where
				$$ D_y = \{x^{(i)} \, : \, y^{(i)} = y\}$$
				\item Estimate $\pi_i$'s $ (i.e. \ \hat \pi_i = \frac{|D_i|}{n})$ and the $P_i$'s for each class
				using maximum likelihood estimation. Obtaining $\hat{\pmb{\pi}}, \hat{\pmb{P}}$
				\item classifier $\hat f$ is Bayes classifier for distribution $P_{\hat \theta}$ corresponding
				to parameter estimates:
				$$ \hat{\pmb{\theta}} = (\hat{\pmb{\pi}}, \hat{\pmb{P}}) $$
			\end{enumerate}
			
			\noindent to classify,
			$$ \hat{f}(x) = arg_y \, max \ \pi_y \cdot P_y(x)$$
			
			TODO finish this. important. Lecture 2 McInerney 
		\subsection*{F. Partitioning Data}
			TODO
		\subsection*{G. Finding the G-spot: Overfitting vs. Underfitting}
			TODO
		\subsection*{H. Feature Scaling}
			TODO
			
			
\section*{II. The Methods: Supervised Learning}

\section*{1. k Nearest Neighbors (k-NN)}
	\subsection*{Layman definition:} 
	Non-parametric method used for classification \textit{and} regression. Given labeled training data
	$\mathcal{X}, \mathcal{Y} $ and $\vec{x} \not \in \mathcal{X}$, find the k nearest (measured by some norm, 
	default is euclidean) $ \vec{x}_j \in \mathcal{X} $ along with corresponding $y_j \in \mathcal{Y}$. Denote the 
	collection of the labels of the k nearest neighbors to $\vec x$ as $Y_{\vec x}^k $. Then we have, for 
	
	\indent \textbf{classification:} output $y \in Y_{\vec x}^k $ such that $y$ appears the most times in $Y_{\vec x}^k $. 
	\indent If two $y$ appear the same number of times, chose $y$ arbitrarily. 
	
	\indent \textbf{regression:} output the average of all $y \in  Y_{\vec x}^k  $ 
	
	\subsection*{Issues and How to Deal with Them}
		\begin{enumerate}
			\item \textbf{Choosing the optimal value of k: } A very small value of k leads to overfitting while a large
			value of k leads to underfitting. Let $\hat f^i$ be the i-NN classifier. We want 
			$k = arg_i \,min( \textrm{err}_D(\hat f^i) )$ where $D$ is the test set. Finding the optimal k
			is usually done by graphing training error and test error as a function of k then chooing k where test error
			flattens out as k increases. 
			\item \textbf{Speeding up k-NN Search:} use k-D Trees (see next section).
		\end{enumerate}
		
	
	\subsection*{Improving the Method}
		TODO.. is it possible?
	\subsection*{Pros vs. Cons}
		\noindent \textbf{Pros} 
		\begin{itemize}
			\item No preprocessing 
			\item No assumptions for underlying distribution (non-parametric)
			\item easy to understand and implement
		\end{itemize}
		
		\noindent \textbf{Cons}
		\begin{itemize}
			\item predictions are very costly, requiring a full scan through the data. Each prediction runs in
				$O(dN)$ ($d$ dimensions, N data set size). Note that methods exist to speed up search.
			\item All the data (or some large subset of it) must be kept in memory for every prediction. This
				can be prohibitive for large N.
			\item How do you compare inputs with non ordered values? Which is nearer to green, yellow or 
				blue? 
		\end{itemize}
	
	\subsection*{Keep in Mind}
		\begin{itemize}
			\item k-NN classifier is often successful where each class has many possible prototypes, and the 
				decision boundary is very irregular
			\item k-NN is likely to do poorly with data having only a few relevant features and lots of irrelevant features 
			\item All features are equally important to k-NN classifier. This becomes an issue when features are 
			poorly scaled, leading to the effective marginalization of certain features. 
		\end{itemize}
	
		
\section*{Detour Uno: K Dimensional Trees  or K-D Trees }
	\subsection*{Motivation}
		With 1-dimensional data we can speed up k-NN by sorting the data set and running binary search for 
		each prediction. This gives runtime of $O(log(N))$ for each prediction, pretty damn good.  How can we 
		generalize the binary tree data structure for n-dimensions? In comes k-D Trees. 
	\subsection*{Layman Definition:} 
		A  k-D Tree is a space-partitioning data structure for organizing points in a k-dimensional space. A leaf may 
		have 1 or more data points. It's instructive to see a simple algorithm for building a k-D Tree. (note that 
		there are many possible algorithms, one of which results in the popular ML method called Decision Tree).  \\ 

		\noindent \textbf{The algorithm:} Given a set of points $\mathcal X \subset \mathbb{R}^d$ and a desired 
		leaf number $n$ (if none is given $n = $ N; this is the k-D tree used to speed up k-NN search). 
		TODO how to deal with repeats??
	
		\begin{enumerate}
			\item arbitrarily choose a dimension $j$ s.t. $1 \leq j \leq d$. 
			\item let $m$ equal the median of $ \{ \vec x_j \ | \ \vec x \in \mathcal X \} $
			\item partiton the data into sets L,R s.t. \\
				L $:= \{\vec x \ | \ \vec x_j \leq m \}$\\
				R $:= \{\vec x \ | \ \vec x_j > m \}$
			\item Recursively apply steps 1-3 on sets L and R until tree has $n$ leaves. 
		\end{enumerate} 
	
	\subsection*{k-NN + k-D Trees}
		For 1-NN, we build a k-D Tree with N (number of data points) leaf nodes. To make a prediction given 
		$\vec x $ we can greedily route $\vec x$ down the nodes of the tree, going left or right depending on 
		if the node condition is satisfied, until reaching a leaf node. Return the $y$ of the data point in that leaf node.  \\
	
		\noindent IMPORTANT NOTE:  the above algorithm is an example of where a greedy algorithm fails. 
		It does not necessarily yield the nearest neighbor, it yields some near neighbor. It is very possible for 
		the nearest neighbor to be in the other subtree. The diagram below shows an example of this pitfall. \\
		
		There are several ways to resolve this issue. One way is to introduce additional condition(s) that must 
		be satisfied at each inner node. Another way is to find nearest neighbor in the current leaf, denote its 
		distance by $r$, and draw a circle of radius $r$ around the query point. Search for leaves whose domain 
		intersect the circle, and search for NN in those leaves. If another point is a distance $r' < r$ from query 
		point, then that point becomes new NN and a new circle of radius $r'$ is drawn. The search is repeated 
		until no nearer neighbor is found.  \\
		
		The k-D Tree method for k-NN $k \geq 2$ is a bit more involved and, as always, there are several different
		approaches. One of the simplest approaches is to build a tree with leaves with a minimum of $k$ data points.
		To make predictions, route query point $\vec x$ down the tree until reaching a leaf. Pick the data point in 
		the leaf furthest from the query point. Let $r$ denote the distance. Then draw a circle of radius $r$ and repeat
		the steps in 1-NN, the difference being the circle must always maintain $k$ points inside. The limitation of
		having leaves of size at least $k$ can be prohibitive for large $k$. For other, perhaps better, methods, 
		google them. 
		
	
	TODO draw tree, show image of defeatest search  

	
\section*{2. Decision Trees}
	\subsection*{Motivation}
		Given labeled training data $\mathcal{X}$, along with labels $\mathcal{Y} = \{1, 2, ... , k\} $ 
		the simplest classifier $\hat f$ we can build is: for each label $y_j$  
		$$P(Y=y_j \ | \ \vec x) = P(Y=y_j) :=  \frac{ \sum\mathbbm{1} \{y^{(i)} = y_j \} }{N}  $$ 
		Although the method is simple, easy to compute, requires no memory, and is easy to understand, it's a piece of shit. 
		$\hat f$ leaves  a lot of valuable information on the table each time it makes a prediction. The relation 
		$P(Y=y_i \ | \ \vec x) = P(Y=y_i)$ shows this clearly. This says the label is independent of the query vector.
		But this feels wrong. The feature values of $\vec x$ must tell us \textit{something};
		they should make $y_i$ more, or less, likely. We should expect $P(Y=y_i \ | \ \vec x) \not = P(Y=y_i)$ to 
		hold in most cases (notice that changing the prior due to some new information to get a posterior is a
		Bayesian approach). So, how do we extract information from $\vec x$? We make the same assumption as in 
		k-NN: data points near each other have similar $y$ values. This gives rise to the following idea:
		What if we partition $\mathcal{X}$ into $m$ regions where most points in a region have the same $y$ value. 
		By doing this, we have that for each region
		$$\exists i \ \ 1 \leq i \leq d \quad \textrm{ s.t. } \quad P(Y = y_i \ | \ \vec x) \approx 1 $$ 
		This is the idea behind Decision Trees. Stop and think, what is the above saying? It is saying our prediction
		will be right almost all of the time. If this gives you an odd feeling that we are 'cheating the system', 
		you are not alone. In fact, you'd be correct. We seem to extract \textit{too} much information from 
		$\mathcal{X}$. We cannot believe everything $\mathcal X$ tells us. Data is usually naturally noisy, 
		meaning that even the best possible classifier will still be wrong some of the time. This issue with 
		'over trusting' the data is in fact overfitting. We will see how to ameliorate this issue a bit later but it's 
		important to see that Decision Trees have a strong tendency to overfit. 
		
	
	\subsection*{From k-NN to Decision Trees}
		If you read the above, you can skip this. But it's informative to continue our quixotic quest for the 
		\textit{most accurate, the most fastest, the most least-memory-userest, and the most versatilest} 
		learning method on the face of the earth. \\
		
		\noindent k-NN with k-D Trees is a definite improvement over 
		regular k-NN. However, it still has considerable limitations. Data must be kept
		 in memory for every prediction. Moreover, how do we deal with a data set where a vector contains both 
		ordered and non ordered points (e.g. $\vec x_i \in \{ \textrm{Blue, Green, Yellow} \}$ and $\vec x_j \in \mathbb{R}$)?
		To address the memory issue, lets take a moment to think about the assumptions we make when we choose 
		k-NN as our classifier (or regressor). We assume that $ \vec x$'s that are near each other have similar $y$ values.
		In k-NN, using k-D Trees to search for $\vec x$'s nearest neighbors, when we get to a leaf, we know that all points
		$m$ in that leaf are near $\vec x$. They may or may not be the $m$ nearest, but they are nearer than most other points.   
		Does it really matter if we get the $k$ nearest points instead of some $m$ relatively very near points?
		What if, for every leaf, we take the most commonly occurring $y$ value, and use it as representative of that leaf? In other
		words, for every $\vec x$ routed to that leaf, we predict the same value. Is this good enough to produce results similar to 
		those of k-NN? Is the accuracy of the model negatively affected? If so, is there something we can do to compensate? 
		The answers to the last three are no, yes, and yes. \\
		
		So what can we do to compensate? Notice that the algorithm shown in the previous section for building a 
		k-D Tree makes no attempt whatsoever at maximizing model accuracy. It solely partitions the data for 
		efficient searching. What if we could partition the data in such a way that maximizes model accuracy? 
		In come Decision Trees. 
	
	\subsection*{Layman Definition}
		A Decision Tree is a non-parametric method that can be used for classification. Geometrically, 
		it is a k-D Tree where each splitting threshold $t$ is chosen such that leaf uncertainty is minimized (we'll cover 
		uncertainty measures shortly). Again, it is instructive to see a generic algorithm for building a Decision Tree. Note 
		that it is the same algorithm as building a k-D Tree, the only difference is $how$ you pick dimension $j$ and the 
		splitting threshold $t$, which in a generic k-D Tree, is the median.\\
	
		\textbf{The algorithm:} Given a set of points $\mathcal X \subset \mathbb{R}^d$ and a stopping criterion 
			\indent (e.g. a desired leaf number $n$, or leaves must have at least $m$ points)
		\begin{enumerate}
			\item choose a dimension $1 \leq j \leq d$ and splitting threshold $t$ such that leaf uncertainty is minimized. 
			\item partiton the data into sets L,R s.t. \\
				L $:= \{\vec x \ | \ \vec x_j \leq t \}$\\
				R $:= \{\vec x \ | \ \vec x_j > t  \}$
			\item Recursively apply steps 1-2 on sets L and R until stopping criterion is true. 
		\end{enumerate} 
		\indent \indent **note that splitting threshold $t$ need not be numeric. For example, say 
		\indent $\vec x_j \in \{ Blue, \ Yellow, \ Green \} $. $Yellow$ could be a splitting threshold for $j$. 
		\indent L  becomes $\{\vec x \ | \ \vec x_j = t \}$ and R $ \{\vec x \ | \ \vec x_j \not = t \}$ \\ 
		
		\noindent To make a prediction given $\vec x $ we route $\vec x$ down the nodes of the tree, going left or 
		right depending on if the node condition is satisfied, until reaching a leaf node. Assume this leaf node has $m$ 
		data points. Denote the collection of the labels of the $m$ near neighbors to $\vec x$ as $Y_{\vec x}^m $. 
		Output $y \in Y_{\vec x}^m $ such that $y$ appears the most times in $Y_{\vec x}^m $ (plurality of y). 
		If more than one $y$ appear the same number of times, chose $y$ arbitrarily. \\
		
		How do we compute uncertainty doe? Let $p_{y_i} :=  \frac{ \sum\mathbbm{1} \{y^{(i)} = y_i \} }{n}  $.
		The three most popular uncertainty measures are: \\
		
		Entropy: \indent \indent \indent $ \mathlarger{ \sum_{y \in \mathcal{Y}} p_y \textrm{log} \frac{1}{p_y} } $
		
		Gini: \indent \indent \indent \indent $ \mathlarger{1 - \sum_{y \in \mathcal{Y}} p_y^2 }$
		
		Classification error:  $ \mathlarger{1 - \sum_{y \in \mathcal{Y}} p_y^2 } $
		
		Each measure results in a different tree, so it's important to chose the measure that makes sense for your
		data. You could also just try them all and chose the one that results in the lowest classification error. 

	
	\subsection*{Notes:}
		After building the tree, we can discard the training data. To make predictions we only need the condition at each 
		inner tree node and the predicted $y$ value at each leaf node. Decision Trees solve the memory issue with k-NN.  \\

		\noindent You may be wondering, how exactly the optimal $j,t$ values are determined at each node. Step 1 is in fact 
		the most important and time consuming step in the algorithm. Note that for each $j$, there are infinite possible 
		$t$ splitting values. To address this issue, we can implement the following algorithm for picking good values for $j, t$.
		Given some subset size N of training data: \\ 
	
		\noindent for each $j$, sort $\vec x_j^{(i)}$  for all  $\ 1 \leq i \leq N$. Set $t$ equal to the midpoint between $ith$ and 
		$i$th $+1$ element in sorted the sorted $\vec x_j^{(i)}$ for $i \in [1,N-1]$. Check uncertainty with these values of $j,t$. 
		Return $j, t$ with minimum uncertainty. \\ 
	
		In python pseudocode:
	
		\begin{lstlisting}
	min_list = []
	for j in range(0, d):
   	    for i in range(0, N):
       		list.append(X[i][j])
    	    list = list.remove_duplicates().sort()
	    
	    min_uncert = 1
  	    for i in range(0, len(list)-1):
	    	t = (list[i] + list[i+1]) / 2.0
		tmp = calculate_uncert(j,t) 
		if tmp <= min_uncert:
		    min = (j,t,tmp)
	    min_list.append(min)
	return min(min_list, key = lambda g:g[2])[:-1]
		
		\end{lstlisting} 
		**this is a shit algorithm. It's just here to give you an idea of how it could work. \\ \\
		
		Another important aspect to note is that, when we interpret the splits as hyperplanes, our splitting algorithm
		limits the hyperplanes to be perpendicular to the splitting dimension's axis. What if we removed this limitation?
		That is, instead we denoted our splits by $\vec \theta \cdot \vec x \leq t$ (we chose $\vec \theta, \ t $ to minimize
		uncertainty, as before) instead of $\vec x_j \leq t$. These more complex splits are called surrogate splits. 
		At first, it seems the increased generality of surrogate splits would lead to a much better model. It turns out 
		this is not the case. The simple splitting method can achieve very similar results to surrogate splits, 
		the only catch is that it requires a bigger tree. This makes sense if you think about it. Since
		we measure decision tree model complexity by tree size, then surrogate splits are theoretically 
		superior. However, computational costs can be prohibitively expensive. Hence the simple splitting method is
		the standard for building the tree. There are cases where looking into surrogate splits can be beneficial, so 
		don't be too quick to rule them out. 
	
	\subsection*{Issues and How to Deal with Them}
		\begin{enumerate}
			\item \textbf{Tree size} A very large tree (i.e. many leaf nodes) leads to overfitting, while a small tree
			leads to underfitting. So when do you stop building the tree? A popular method is to build a very large tree, 
			and then 'prune it'. That is, remove a leaf node and test prediction accuracy against a hold-out set. 
			If model accuracy is improved, the leaf node is truly removed. This is continued until removing any leaf 
			decreases accuracy. 
		\end{enumerate}
	\subsection*{Pros vs. Cons}
		\textbf{Pros}
		\begin{itemize}
			\item Easy to interpret results and present resulting tree to a layman
			\item Do not have to keep training data around for predictions
			\item Very fast predictions
			\item Can deal with data containing ordered and non-ordered values.
			\item Ignores features that do not contribute to label, since it won't split at these features
		\end{itemize}
	
		\noindent \textbf{Cons}
		\begin{itemize}
			\item It is very easy to overfit the data, even after pruning.
			\item Needs a lot of data to perform well.
		\end{itemize}
	
	\subsection*{Keep in Mind}
		Overfitting is a serious issue that must be addressed when using a DT in any serious work. This may require
		tuning the pruning process. Also remember there are methods (most popular are boosting and random forests)
		that can considerably improve tree performance. \\
	
\section*{3. Regression Trees}
	This is a very, very brief section since Regression Trees are literally just Decision Trees with a different 
	uncertainty measure.
	\subsection*{Motivation}
		 What if we need a model that is a regression (i.e. it must predict continuous and real values)? For example, we 
		 must predict house values given room numbers and square feet. In this simple example we can use a regular
		 linear regression. But we usually are trying to solve much more complex problems where dimensions are very 
		 high and relationships are highly non linear. We could then resort to a non-linear regression. However, notice 
		 that we have a single regression over the whole data space. Could we split the data space in 2 and fit a 
		 better regression on each subset of data? Most likely it's a yes. What if we split each subset further, and fit 
		 regressions on those? This is the idea behind the regression tree. 
	\subsection*{Definition}
		I will skip the definition since it's the same as Decision Trees. I will just briefly address the different 
		uncertainty measure, which, in the simples case, is the variance, as shown below.
		$$ \frac{1}{n} \sum_{i=1}^{n} (\vec x_j^{(i)} - \mu_j)^2 $$
		where $n$ is the data points in that leaf.
		To make a prediction, we route $ \vec x$ down to a leaf and return the sample mean of $Y$ in that leaf. 
		Note that the resulting model is piecewise constant. We are not truly fitting a line at each node. Although this 
		piecewise constant model is easy to understand and simple to compute, the prediction accuracy of these models often
		lags behind smoother models. It is straightforward to see how we can a fit line at each node, but the costs are usually
		prohibitively high. There are several algorithms (M5, GUIDE) that do some attempt at smoothing to get better results. \\
		 
		\noindent Regression tress and decision trees are collectively referred to as CART 
		(Classification and Regression Trees).
		
		
\section*{4. Bayes Classifier}
	\subsection*{Motivation}
		Consider the following imaginary scenario. We are faced with the task of predicting sex based only on height 
		measurements. Since we are dealing with one dimensional data, we decide to plot the points, labelling male 
		values with an x and female values with an o. When we look at the points on the line, we see that the o's cluster 
		around one value and the x's cluster around another. We then decide to build a histogram of the male and 
		female height values. We see that both histograms form a rough bell curve. If we have some basic knowledge 
		of probability theory, we'd say, "hey, it seems like male histogram delineate a gaussian curve and women's 
		delineate another. If we modeled height as a random variable drawn from a gaussian distribution, we could 
		make some pretty accurate predictions." This is the rough idea behind probabilistic models. 
		
	\subsection*{The search continues: from geometry to probability}
		k-NN and Decision Trees are good and all, but they aren't perfect. Being the perfectionists and idealists that we are, 
		we have this feeling in our gut that the perfect method is still out there, restlessly waiting to be found, anxious to
		share with us the world's knowledge, eager to bloom and display the true beauty of data. Let us take a step back and
		again ask the fundamental learning question: given $\mathcal X, \ \mathcal Y$ and $\vec x \not \in \mathcal X$ 
		how can we predict the correct $y \in \mathcal Y$ associated with $\vec x$? In k-NN and Decision Trees, the only
		assumption we made is that points near each other have similar $y$ 
		values. What if we take on a more probabilistic view of the world? Namely, what if we assume there exists some 
		probability distribution $\mathcal P$ from which all points $(\vec x, y)$ are drawn? This assumption would
		allow us to use the full power of probabilistic tools to tackle our learning question. Armed with these, maybe we
		can finally build the ultimate optimal classifier.
		
	\subsection*{Layman Definition}
		(You should go back and review the sections on probability and statistics if any of this is not immediately obvious) \\ \\
		Remember the Bayes Optimal Classifier? 		
		$$ f^*(\vec x) = arg_y \, max \ P(Y = y\, | \, \vec x ) = arg_y \, max \ P(\vec x \, | \, Y=y)P(Y = y)$$
		In summary, it's a classifier that returns the label with the highest probability conditional on $\vec x$. 
		The Bayes Optimal Classifier we'll use is an estimator of the \textit{true} Bayes Optimal Classifier. 
		To see this, consider the following case. We have a data set $D = \{ (\vec x^{(i)}, y^{(i)})\}_{i=1}^n$ of
		i.i.d. samples from $P_{\pmb \theta} \in \mathcal P$ where $\mathcal P$ is a generative statistical model with 
		parameter space $\Theta$. However, we don't know $P_{\pmb \theta}$. How do we go about estimating it? 
		Estimating $\pmb \theta$ is what we call model fitting. There are several methods to solve this problem. 
		A popular and generally very robust method is maximum likelihood estimation (MLE). Notice 
		that since $\mathcal P$ is a generative model, we need to estimate both the class priors $P(Y=y)$ and the class 
		conditionals $P(X=\textbf{x} \, | \, Y=y)$. Once we have $P_{\hat{ \pmb \theta}}$, we build a Bayes 
		Optimal Classifier $\hat f$ for it. \\
		
		\noindent The key thing to realize here is that $f^*$ is the Bayes Optimal Classifier for $P_{\pmb \theta}$ while
		$\hat f$ is the Bayes Optimal Classifier for $P_{\hat{ \pmb \theta}}$. Also notice that using MLE requires us to
		set a length (i.e. number of parameters) for $\pmb \theta$ and, perhaps more importantly, requires us to chose 
		an underlying distribution (gaussian, poisson, etc). We could chose $P_{\hat{\pmb \theta}}$ to have 10 
		parameters (five labels) and be normally distributed, but what if the data's true underlying distribution has 
		14 (seven labels) parameters$P_{{\pmb \theta}}$ is strongly bimodal? Our classifier would not perform very
		well. This is the main drawback from using probabilistic models, they often require us to make assumptions that
		are above our pay grade. There are other estimation models that do not as assumption-needy, such as 
		non-parametric and graphical models. These are usually more involved and also have their drawbacks 
		(non-parametric often performs poor in high dimensions). After studying the data however,  we can often 
		glean valuable information that allows us to make some pretty good assumptions about the underlying 
		probability distribution. In these cases, Baysian Classifiers really shine.
		\\ 
		
	\subsection*{How to use:}
		The definition offered above wasn't formatted in 'definition' format. So I will show an example of how 
		to actually build a Bayesian Classifier. Say we have
		a data set $D = \{ (\vec x^{(i)}, y^{(i)})\}_{i=1}^n$ regarded as an i.i.d. sample.
		\begin{enumerate}
			\item (We don't actually do this in practice. We just do step 2 directly. But it's useful to think of it this way.) 
				Partition $\{\vec x^{(i)}\}_{i=1}^{n}$ into $D_1, D_2, ... D_k$ where
				$$ D_y = \{\vec x^{(i)} \, : \, y^{(i)} = y\}$$
			\item Estimate $P_{\pmb \theta}$. That is, estimate the class priors $\pi_i$'s 
				$ (i.e. \ \hat \pi_i = \frac{|D_i|}{n})$ and the class conditionals
				$P_i$'s (i.e. $P(\vec x \, | \, Y)$) for each class using maximum likelihood estimation,
				obtaining $\hat{\pmb{\pi}}, \hat{\pmb{P}}$. We have several options for modeling the class conditionals. 
				We could use non-parametric models, graphical models, or simple distribution models (e.g. multivariate
				gaussian). The first two are above my clearance so for this example, let's assume multivariate gaussian. 
				Using MLE to fit $\pmb \theta$, we arrive at 
				$$ \hat{\mu}_i = \frac{1}{|D_i|}\sum_{i=1}^{|D_i|} \vec x^{(i)} \quad \quad
				\hat{\Sigma}_i = \frac{1}{|D_i|}\sum_{i=1}^{|D_i|} (\vec x^{(i)} - \hat \mu_i)(\vec x^{(i)} - \hat \mu_i)^T $$
			\item  $ \hat{f}(x) = arg_y \, max \ \pi_y \cdot P_y(x)$ is Bayes classifier for distribution $P_{\hat \theta}$ 
				corresponding to parameter estimates: $ \hat{\pmb{\theta}} = (\hat{\pmb{\pi}}, \hat{\pmb{P}}) $
		\end{enumerate}	
		
	\subsection*{Naive Bayes Classifier: The Slutty Sister}
		Sometimes, due to various circumstances (complexity of data, need for speed, and whatnot), we are 
		cornered into making some further assumptions (jk, sometimes we'd actually want to make the naive
		assumption). The Naive Bayes Classifier assumes that individual features 
		are independent of each other given the class label. Hence the term naive. 
		$$ f(\vec x) =  arg_y \, max \ \prod_{j=1}^{d}P(\vec x_j \, | \, Y=y)P(Y = y)$$
		Naive Bayes Classifier is computationally very simple and it's quick to code. However, if the data
		set in question has correlated features (many do) it can give very bad estimates. Extending the
		"How to use" section to Naive Bayes is straightforward so I won't rewrite it. \\

	\subsection*{Issues and How to Deal with Them}
		Choosing a model $P_\theta$, choosing an estimation method. 
	\subsection*{Pros vs. Cons}		
		\textbf{Pros}
		\begin{itemize}
			\item straightforward recipe for building a classifier
			\item Allows us to leverage domain knowledge of class conditional distributions
			\item Can be very efficient when number of labels is large
			\item If our estimated distribution is close the real distribution, than Bayes Classifier
				can perform well with a small amount of training data. 
		\end{itemize}
		\textbf{Cons}
		\begin{itemize}
			\item Classifier assumes data comes from estimated distribution $P_{\hat \theta}$, which 
				is generally not true. 
			\item A more philosophical critique is that there is wasted effort in defining the joint
				probabilities. Since we are solving a classification problem, all that matters 
				is the boundary
		\end{itemize}
	\subsection*{Keep in Mind}
		Although the distribution assumptions we must make for Bayes Classifier are a great
		weakness, they can at times be a great strength. If our assumptions are approximately correct, 
		Bayes Classifier needs much less data than k-NN and CART to achieve low error rates. When
		training data is scarce and we have some prior knowledge of it's structure, then Bayes Classifer
		could very well be the best choice.

\section{Detour Dos: Discriminative vs. Generative}
	Now that we have learned three popular and powerful methods, k-NN, CART, and Bayes Classifier, 
	we should start to have somewhat of a feel for what a learning method truly is. We often hear the term 
	"extract information from the data", but what does it actually mean? In classification problems, it means 
	we can characterize each of the $k$ classes by a set $\mathcal C_k \subset \mathbb{R}^d$ where $d$ 
	is the number of features (the sets $\mathcal C_k$ are usually non-convex and disjoint). Thus, given a
	new observation $\vec x$ (i.e. a point in $\mathbb{R}^d$), we assign it to class $k$ such that 
	$\vec x \in \mathcal C_k$. Although we have not thought about it in this way, all the methods we have
	 seen partition $\mathbb{R}^d$ into $k$ subsets, one for each label. They all do so in a different manner.
	 If we compare the methods seen so far, to which method would you say k-NN is most similar to, Decision 
	 Trees (DT) or Bayes Classifier (BC)? k-NN is clearly closer to DT than BC. In k-NN and DT we only 
	 assume points close to each other have similar labels; we directly build the decision boundary without
	 many prior assumptions. BC, on the other hand, requires us to make serious assumptions about the 
	 underlying distribution of the data, then we fit a model, then we make predictions. k-NN and Decision
	 Trees are examples of \textit{discriminative classifiers} while Bayes Classifier is an example of a
	 \textit{generative classifier}. 
	








		
\end{document}